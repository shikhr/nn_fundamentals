{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5792f2f5",
   "metadata": {
    "papermill": {
     "duration": 0.007047,
     "end_time": "2024-06-08T20:00:45.403167",
     "exception": false,
     "start_time": "2024-06-08T20:00:45.396120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# exercise\n",
    "\n",
    "Build your own GPT-4 Tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929b9b1a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-08T20:00:45.417578Z",
     "iopub.status.busy": "2024-06-08T20:00:45.417133Z",
     "iopub.status.idle": "2024-06-08T20:01:01.971101Z",
     "shell.execute_reply": "2024-06-08T20:01:01.969832Z"
    },
    "papermill": {
     "duration": 16.564754,
     "end_time": "2024-06-08T20:01:01.974363",
     "exception": false,
     "start_time": "2024-06-08T20:00:45.409609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\r\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tiktoken\r\n",
      "Successfully installed tiktoken-0.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "import regex as re\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777c8bcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:01:01.991488Z",
     "iopub.status.busy": "2024-06-08T20:01:01.990431Z",
     "iopub.status.idle": "2024-06-08T20:01:03.782018Z",
     "shell.execute_reply": "2024-06-08T20:01:03.780812Z"
    },
    "papermill": {
     "duration": 1.803228,
     "end_time": "2024-06-08T20:01:03.784961",
     "exception": false,
     "start_time": "2024-06-08T20:01:01.981733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-08 20:01:03--  https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 185768 (181K) [text/plain]\r\n",
      "Saving to: 'taylorswift.txt'\r\n",
      "\r\n",
      "taylorswift.txt     100%[===================>] 181.41K  --.-KB/s    in 0.1s    \r\n",
      "\r\n",
      "2024-06-08 20:01:03 (1.72 MB/s) - 'taylorswift.txt' saved [185768/185768]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! wget \"https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea752ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:01:03.803595Z",
     "iopub.status.busy": "2024-06-08T20:01:03.803154Z",
     "iopub.status.idle": "2024-06-08T20:01:03.812676Z",
     "shell.execute_reply": "2024-06-08T20:01:03.811497Z"
    },
    "papermill": {
     "duration": 0.022722,
     "end_time": "2024-06-08T20:01:03.815375",
     "exception": false,
     "start_time": "2024-06-08T20:01:03.792653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = open(\"taylorswift.txt\").read()\n",
    "text = re.sub('\\n', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125126cb",
   "metadata": {
    "papermill": {
     "duration": 0.007378,
     "end_time": "2024-06-08T20:01:03.830461",
     "exception": false,
     "start_time": "2024-06-08T20:01:03.823083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Step 1\n",
    "\n",
    "Write the `BasicTokenizer` class, with the following three core functions:\n",
    "\n",
    "- `def train(self, text, vocab_size, verbose=False)`\n",
    "- `def encode(self, text)`\n",
    "- `def decode(self, ids)`\n",
    "\n",
    "Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file `tests/taylorswift.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1bdb383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:01:03.847609Z",
     "iopub.status.busy": "2024-06-08T20:01:03.847183Z",
     "iopub.status.idle": "2024-06-08T20:01:03.857492Z",
     "shell.execute_reply": "2024-06-08T20:01:03.856222Z"
    },
    "papermill": {
     "duration": 0.022357,
     "end_time": "2024-06-08T20:01:03.860298",
     "exception": false,
     "start_time": "2024-06-08T20:01:03.837941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def count_pairs(self, tk, pairs=None):\n",
    "        pairs = {} if pairs is None else pairs\n",
    "        for i in range(1, len(tk)):\n",
    "            pairs[(tk[i-1], tk[i])] = pairs.get((tk[i-1],tk[i]), 0) + 1\n",
    "        return pairs\n",
    "    \n",
    "    def merge(self,tk, target, idx):\n",
    "        merged = []\n",
    "        i = 0\n",
    "        while i < len(tk):\n",
    "            if i < len(tk)-1 and (tk[i], tk[i+1]) == target:\n",
    "                merged.append(idx)\n",
    "                i+=2\n",
    "            else:\n",
    "                merged.append(tk[i])\n",
    "                i+=1\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a3746a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:01:03.877710Z",
     "iopub.status.busy": "2024-06-08T20:01:03.877295Z",
     "iopub.status.idle": "2024-06-08T20:01:03.891663Z",
     "shell.execute_reply": "2024-06-08T20:01:03.890461Z"
    },
    "papermill": {
     "duration": 0.026073,
     "end_time": "2024-06-08T20:01:03.894238",
     "exception": false,
     "start_time": "2024-06-08T20:01:03.868165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.encoding = {i: bytes([i]) for i in range(256)}\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        tokens = list(bytes(text, \"UTF-8\"))\n",
    "        ids = list(tokens)\n",
    "        merge_cnt = vocab_size-256\n",
    "        new_id = 256\n",
    "        for i in range(merge_cnt):\n",
    "            pairs = self.count_pairs(ids)\n",
    "            target = max(pairs, key = pairs.get)\n",
    "            ids = self.merge(ids, target, new_id)\n",
    "            self.merges[target] = new_id\n",
    "            self.encoding[new_id] = self.encoding[target[0]] + self.encoding[target[1]]\n",
    "            new_id+=1\n",
    "        \n",
    "        print(f'merges done: {len(self.merges)}')\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_bytes = list(bytes(text, \"UTF-8\"))\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            stats = self.count_pairs(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            ids = self.merge(ids, pair, idx)\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text_bytes = b\"\".join(self.encoding[idx] for idx in ids)\n",
    "        return text_bytes.decode('utf-8', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b858ce26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:01:03.911171Z",
     "iopub.status.busy": "2024-06-08T20:01:03.910768Z",
     "iopub.status.idle": "2024-06-08T20:01:03.915972Z",
     "shell.execute_reply": "2024-06-08T20:01:03.914810Z"
    },
    "papermill": {
     "duration": 0.016698,
     "end_time": "2024-06-08T20:01:03.918485",
     "exception": false,
     "start_time": "2024-06-08T20:01:03.901787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = BasicTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df186016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:01:03.935768Z",
     "iopub.status.busy": "2024-06-08T20:01:03.934978Z",
     "iopub.status.idle": "2024-06-08T20:06:57.264771Z",
     "shell.execute_reply": "2024-06-08T20:06:57.263582Z"
    },
    "papermill": {
     "duration": 353.348188,
     "end_time": "2024-06-08T20:06:57.274291",
     "exception": false,
     "start_time": "2024-06-08T20:01:03.926103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merges done: 4744\n",
      "CPU times: user 5min 53s, sys: 91 ms, total: 5min 53s\n",
      "Wall time: 5min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t.train(text, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70cc5922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:06:57.291301Z",
     "iopub.status.busy": "2024-06-08T20:06:57.290917Z",
     "iopub.status.idle": "2024-06-08T20:06:57.334181Z",
     "shell.execute_reply": "2024-06-08T20:06:57.332929Z"
    },
    "papermill": {
     "duration": 0.055353,
     "end_time": "2024-06-08T20:06:57.337207",
     "exception": false,
     "start_time": "2024-06-08T20:06:57.281854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "['In ', 'The New Yorker ', 'in 2011', ', Swift ', 'said she ', 'identifies ', 'as a songwrit', 'er ', 'fir', 'st', ': \"', 'I ', 'write ', 'song', 's, and ', 'my ', 'voice ', 'is ', 'just ', 'a ', 'way ', 'to ', 'get ', 'th', 'o', 'se ', 'lyrics ', 'ac', 'ross', '\". ', 'Her ', 'personal ', 'experi', 'ences ', 'were ', 'a ', 'common ', 'inspir', 'ation ', 'for her ', 'early ', 'song', 's, which ', 'help', 'ed her ', 'n', 'av', 'ig', 'ate ', 'li', 'fe', '.']\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"In The New Yorker in 2011, Swift said she identifies as a songwriter first: \"I write songs, and my voice is just a way to get those lyrics across\". Her personal experiences were a common inspiration for her early songs, which helped her navigate life.\"\"\"\n",
    "out = [t.decode([y]) for y in t.encode(string)]\n",
    "print(len(out))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7984e9",
   "metadata": {
    "papermill": {
     "duration": 0.007898,
     "end_time": "2024-06-08T20:06:57.352939",
     "exception": false,
     "start_time": "2024-06-08T20:06:57.345041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Step 2\n",
    "\n",
    "Convert you `BasicTokenizer` into a `RegexTokenizer`, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\n",
    "\n",
    "```\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb0e541",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:06:57.370548Z",
     "iopub.status.busy": "2024-06-08T20:06:57.370127Z",
     "iopub.status.idle": "2024-06-08T20:06:57.386426Z",
     "shell.execute_reply": "2024-06-08T20:06:57.385349Z"
    },
    "papermill": {
     "duration": 0.028104,
     "end_time": "2024-06-08T20:06:57.389022",
     "exception": false,
     "start_time": "2024-06-08T20:06:57.360918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegexTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.encoding = {i: bytes([i]) for i in range(256)}\n",
    "        self.regx = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        \n",
    "        \n",
    "    def train(self, text, vocab_size):\n",
    "        chunks = re.findall(self.regx, text)\n",
    "        tokens = [list(bytes(t, \"utf-8\")) for t in chunks]\n",
    "        mergecnt = vocab_size-256\n",
    "        new_idx = 256\n",
    "        ids = list(tokens)\n",
    "        for i in range(mergecnt):\n",
    "            pairs = {}\n",
    "            for tks in ids: self.count_pairs(tks, pairs)\n",
    "            target = max(pairs, key=pairs.get)\n",
    "            ids = [self.merge(tks, target, new_idx) for tks in ids]\n",
    "            self.merges[target] = new_idx\n",
    "            self.encoding[new_idx] = self.encoding[target[0]] + self.encoding[target[1]]\n",
    "            new_idx += 1\n",
    "            \n",
    "    def encode_(self, text):\n",
    "        text_bytes = list(bytes(text, \"UTF-8\"))\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            stats = self.count_pairs(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            ids = self.merge(ids, pair, idx)\n",
    "        return ids        \n",
    "    \n",
    "    def encode(self, text):\n",
    "        chunks = re.findall(self.regx, text)\n",
    "        ids = []\n",
    "        for chunk in chunks:\n",
    "            chunk_ids = self.encode_(chunk)\n",
    "            ids.extend(chunk_ids)\n",
    "            \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text_bytes = b\"\".join(self.encoding[idx] for idx in ids)\n",
    "        return text_bytes.decode('utf-8', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e7b7714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:06:57.407733Z",
     "iopub.status.busy": "2024-06-08T20:06:57.407299Z",
     "iopub.status.idle": "2024-06-08T20:06:57.412543Z",
     "shell.execute_reply": "2024-06-08T20:06:57.411309Z"
    },
    "papermill": {
     "duration": 0.018146,
     "end_time": "2024-06-08T20:06:57.415271",
     "exception": false,
     "start_time": "2024-06-08T20:06:57.397125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10b632d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:06:57.432594Z",
     "iopub.status.busy": "2024-06-08T20:06:57.432175Z",
     "iopub.status.idle": "2024-06-08T20:18:02.095711Z",
     "shell.execute_reply": "2024-06-08T20:18:02.094465Z"
    },
    "papermill": {
     "duration": 664.682655,
     "end_time": "2024-06-08T20:18:02.105738",
     "exception": false,
     "start_time": "2024-06-08T20:06:57.423083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 4s, sys: 156 ms, total: 11min 4s\n",
      "Wall time: 11min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t1.train(text, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3144dedf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:18:02.124499Z",
     "iopub.status.busy": "2024-06-08T20:18:02.124076Z",
     "iopub.status.idle": "2024-06-08T20:18:02.134648Z",
     "shell.execute_reply": "2024-06-08T20:18:02.133264Z"
    },
    "papermill": {
     "duration": 0.023451,
     "end_time": "2024-06-08T20:18:02.137299",
     "exception": false,
     "start_time": "2024-06-08T20:18:02.113848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "['In', ' The', ' New', ' Yorker', ' in', ' ', '201', '1', ',', ' Swift', ' said', ' she', ' identifies', ' as', ' a', ' songwriter', ' first', ':', ' \"', 'I', ' write', ' songs', ',', ' and', ' my', ' voice', ' is', ' just', ' a', ' way', ' to', ' get', ' those', ' lyrics', ' ac', 'ross', '\".', ' Her', ' personal', ' experi', 'ences', ' were', ' a', ' common', ' insp', 'iration', ' for', ' her', ' early', ' songs', ',', ' which', ' helped', ' her', ' n', 'av', 'ig', 'ate', ' life', '.']\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"In The New Yorker in 2011, Swift said she identifies as a songwriter first: \"I write songs, and my voice is just a way to get those lyrics across\". Her personal experiences were a common inspiration for her early songs, which helped her navigate life.\"\"\"\n",
    "out = [t1.decode([y]) for y in t1.encode(string)]\n",
    "print(len(out))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ce64fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T20:18:02.155082Z",
     "iopub.status.busy": "2024-06-08T20:18:02.154679Z",
     "iopub.status.idle": "2024-06-08T20:18:02.159462Z",
     "shell.execute_reply": "2024-06-08T20:18:02.158289Z"
    },
    "papermill": {
     "duration": 0.016547,
     "end_time": "2024-06-08T20:18:02.161809",
     "exception": false,
     "start_time": "2024-06-08T20:18:02.145262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list(t1.encoding.items())[250:]\n",
    "# t1.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4007521f",
   "metadata": {
    "papermill": {
     "duration": 0.007789,
     "end_time": "2024-06-08T20:18:02.177625",
     "exception": false,
     "start_time": "2024-06-08T20:18:02.169836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4246d",
   "metadata": {
    "papermill": {
     "duration": 0.007722,
     "end_time": "2024-06-08T20:18:02.193314",
     "exception": false,
     "start_time": "2024-06-08T20:18:02.185592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1040.332041,
   "end_time": "2024-06-08T20:18:02.625409",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-08T20:00:42.293368",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
